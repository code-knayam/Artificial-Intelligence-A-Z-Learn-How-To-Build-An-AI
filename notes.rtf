{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf100
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;\red27\green31\blue34;\red0\green0\blue0;\red255\green255\blue255;
\red12\green69\blue100;\red68\green68\blue67;}
{\*\expandedcolortbl;;\cssrgb\c14118\c16078\c18039;\cssrgb\c0\c0\c0\c84314;\cssrgb\c100000\c100000\c100000;
\cssrgb\c2353\c34118\c46667;\cssrgb\c33725\c33725\c33333;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\paperw11900\paperh16840\margl1440\margr1440\vieww6420\viewh16000\viewkind0
\deftab720
\pard\pardeftab720\sl600\sa320\partightenfactor0

\f0\i\b\fs36 \cf2 \expnd0\expndtw0\kerning0
Artificial-Intelligence-A-Z-Learn-How-To-Build-An-AI\
({\field{\*\fldinst{HYPERLINK "https://www.superdatascience.com/artificial-intelligence/"}}{\fldrslt https://www.superdatascience.com/artificial-intelligence/}})
\i0\fs30 \

\fs34 Reinforcement Learning -
\fs30 \

\b0 The 
\b Agent
\b0  does an 
\b Action
\b0  on an 
\b environment, 
\b0 which in turns causes a change in the
\b  state
\b0  thereby returning a 
\b reward
\b0  to the agent. By doing this, the agent is learning about the environment. Getting to know which action causes what type of reward.
\fs28 \
\pard\pardeftab720\sl860\partightenfactor0

\b \cf3 \cb4 Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural\'a0Networks
\b0\fs30 \cf2 \cb1 \
\pard\pardeftab720\sl600\sa320\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0"}}{\fldrslt \cf2 https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0}}\
\pard\pardeftab720\sl600\sa320\partightenfactor0

\b\fs48 \cf2 \
\pard\pardeftab720\sl600\sa320\partightenfactor0

\fs34 \cf2 The Bellman Equation-\

\b0\fs30 s - state\
a- action\
r- reward\
Y (gamma)- discount factor - it discounts the value of the state \
V - value\
s\'92 - next state
\fs24 \cf0 \kerning1\expnd0\expndtw0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardeftab720\pardirnatural\partightenfactor0

\b\fs30 \cf0 V(S) = max(R(s, a) + YV(s\'92))\
\

\b0 Suppose in a maze, the final state has a max reward of 1.\
So a state before that, R(s, a) will be 0 and the next state will be  1 times. So V = 1\
A state before that, the reward will be 0, the value of next state would be 1 ( form above ) and Y = 0.9 ( suppose ). So final V = 0 + 0.9*1 = 0.9.\
Similarly, states before that, 0.81, 0.73, 0.66 \
\
These values help the agent in deciding which way to go based on the value of the state.\

\b\fs48 \cf2 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\sl600\sa320\partightenfactor0

\fs34 \cf2 The \'93Plan\'94-
\fs48 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardeftab720\pardirnatural\partightenfactor0

\b0\fs30 \cf0 \kerning1\expnd0\expndtw0 The values can be replaced with the arrows pointing to the direction in which the Agent should go.  It points from a state with a lower value to a state with a higher value.\
\
\

\b Markov Decision Process - \

\b0 deterministic search - there\'92s 100% chances of happening a thing, that particular option would happen\
\
non-deterministic search - there may be multiple options, not just one option\
\

\i Markov Process - 
\i0 \
markov property, the future states are only the result of present state, doesn\'92t depend on how you got there.\
\

\i Markov Decision Process - 
\i0 \
provide a mathematical framework for modelling decision making in situations where outcomes are partly random and partly under the control of a decision maker.\
what happens from now on doesn\'92t depend on the past\
\
Basically, helps in choosing a choice from multiple possibilities.\
the Belaman equation can be used here along with some new randomness - \
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardeftab720\pardirnatural\partightenfactor0

\b \cf0 V(S) = max(R(s, a) + YV(s\'92))\
\
Now this V(s\'92) = is an average of multiple possibilities from that state.\
From S, agent can go to state - s1, s2, s3\
V(S\'92) = 0.8*V(s1) + 0.1*V(s2)+ 0.1*V(s3)\
0.8, 0.1, 0.1 - examples of what are the chances of a state. can vary\
\
\

\b0 So final equation comes out to be - \

\b V(S) = max( R(s,a) + Y E( P(s,a,s\'92)V(s\'92) )   )\

\b0 \
E -> Sigma\
P - probability of that state happening (0.8, 0.1, 0.1 from above)\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl460\partightenfactor0
\ls1\ilvl0
\f1\i\b\fs36 \cf5 {\listtext	\uc0\u8226 	}{\field{\*\fldinst{HYPERLINK "http://www.cs.uml.edu/ecg/uploads/AIfall14/MDPApplications3.pdf"}}{\fldrslt \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec5 A Survey of Applications of Markov Decision Processes}}
\i0\b0 \cf6 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec6 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardeftab720\pardirnatural\partightenfactor0

\f0\fs30 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
http://www.cs.uml.edu/ecg/uploads/AIfall14/MDPApplications3.pd\
\

\b Policy VS Plan - \

\b0 The previous plan couldn\'92t work since it was following a deterministic approach. But with new randomness and Markov process, that plan is no more applicable. Also the values of state determined can\'92t be used. For determining the new values, the modified Belman Equation needs to be used framed above.\
That becomes the policy which the agent would follow in order to get to the final state.\

\b \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardeftab720\pardirnatural\partightenfactor0

\b0 \cf0 \

\b Adding a \'93Living Penalty\'94 - \

\b0 when a new penalty is added , the policy of the agent changes.\
if the penalty goes higher than the final reward, then it will try to end the game any how. even by jumping into the state that gives a negative reward and ends the game.\
\

\b\fs48 \cf2 \expnd0\expndtw0\kerning0
\
\
\
\pard\pardeftab720\sl600\sa320\partightenfactor0
\cf2 \
}