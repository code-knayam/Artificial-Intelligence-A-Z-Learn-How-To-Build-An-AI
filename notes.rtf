{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf100
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;}
{\colortbl;\red255\green255\blue255;\red27\green31\blue34;\red0\green0\blue0;\red255\green255\blue255;
\red27\green31\blue34;}
{\*\expandedcolortbl;;\cssrgb\c14118\c16078\c18039;\cssrgb\c0\c0\c0\c84314;\cssrgb\c100000\c100000\c100000;
\cssrgb\c14118\c16078\c18039;}
\paperw11900\paperh16840\margl1440\margr1440\vieww7040\viewh16000\viewkind0
\deftab720
\pard\pardeftab720\sl600\sa320\partightenfactor0

\f0\i\b\fs36 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Artificial-Intelligence-A-Z-Learn-How-To-Build-An-AI\
({\field{\*\fldinst{HYPERLINK "https://www.superdatascience.com/artificial-intelligence/"}}{\fldrslt https://www.superdatascience.com/artificial-intelligence/}})
\f1\i0\fs30 \

\f0\fs34 Reinforcement Learning -
\fs30 \

\b0 The 
\b Agent
\b0  does an 
\b Action
\b0  on an 
\b environment, 
\b0 which in turns causes a change in the
\b  state
\b0  thereby returning a 
\b reward
\b0  to the agent. By doing this, the agent is learning about the environment. Getting to know which action causes what type of reward.
\fs28 \
\pard\pardeftab720\sl860\partightenfactor0

\b \cf3 \cb4 \strokec3 Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural\'a0Networks
\b0\fs30 \cf2 \cb1 \strokec2 \
\pard\pardeftab720\sl600\sa320\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0"}}{\fldrslt \cf2 https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0}}\

\f1\b\fs48 \
\pard\pardeftab720\sl600\sa320\partightenfactor0

\f0\fs34 \cf5 \outl0\strokewidth0 The Bellman Equation-\
\pard\pardeftab720\sl600\sa320\partightenfactor0

\b0\fs30 \cf5 s - state\
a- action\
r- reward\
Y (gamma)- discount factor - it discounts the value of the state \
V - value\
s\'92 - next state
\fs24 \cf0 \kerning1\expnd0\expndtw0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\b\fs30 \cf0 V(S) = max(R(s, a) + YV(s\'92))\
\

\b0 Suppose in a maze, the final state has a max reward of 1.\
So a state before that, R(s, a) will be 0 and the next state will be  1 times. So V = 1\
A state before that, the reward will be 0, the value of next state would be 1 ( form above ) and Y = 0.9 ( suppose ). So final V = 0 + 0.9*1 = 0.9.\
Similarly, states before that, 0.81, 0.73, 0.66 \
\
These values help the agent in deciding which way to go based on the value of the state.\

\f1\b\fs48 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 \
\
\
\
\pard\pardeftab720\sl600\sa320\partightenfactor0
\cf2 \
}